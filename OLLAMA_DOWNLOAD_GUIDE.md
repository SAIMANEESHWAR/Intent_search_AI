# ğŸ“¥ Ollama Download & Installation Guide

## Where to Download Ollama

### Official Download Links

**Windows:**
- **URL:** https://ollama.ai/download
- **Direct Download:** https://ollama.ai/download/windows
- **File:** `OllamaSetup.exe` (or similar)

**Mac:**
- **URL:** https://ollama.ai/download
- **Direct Download:** https://ollama.ai/download/mac
- **File:** `Ollama.dmg`

**Linux:**
- **URL:** https://ollama.ai/download
- **Install via command:** `curl -fsSL https://ollama.ai/install.sh | sh`

---

## Installation Location

### Important: Ollama is a System-Wide Application

**Ollama does NOT need to be in your project folder!**

- âœ… **Install Ollama anywhere** - It's a system application
- âœ… **Default location is fine** - Usually `C:\Users\YourName\AppData\Local\Programs\Ollama` (Windows)
- âœ… **Your project stays separate** - Ollama runs independently

### How It Works

```
Your Project Folder (D:\desktop-top\Intent_search_Cine_Ai\)
    â”œâ”€â”€ app.py
    â”œâ”€â”€ rag_generator.py
    â”œâ”€â”€ .env (configures connection to Ollama)
    â””â”€â”€ ... (your project files)

Ollama (System Installation - separate location)
    â”œâ”€â”€ Installed in: C:\Users\YourName\AppData\Local\Programs\Ollama (Windows)
    â”œâ”€â”€ Runs as: Background service
    â”œâ”€â”€ Listens on: http://localhost:11434
    â””â”€â”€ Models stored in: C:\Users\YourName\.ollama\models (Windows)
```

**Your project connects to Ollama via HTTP** - They don't need to be in the same folder!

---

## Step-by-Step Installation

### Step 1: Download Ollama

1. Go to: **https://ollama.ai/download**
2. Click download for your operating system
3. Save the installer file (can save anywhere, like Downloads folder)

### Step 2: Install Ollama

**Windows:**
1. Double-click the downloaded installer
2. Follow installation wizard
3. Ollama will install to: `C:\Users\YourName\AppData\Local\Programs\Ollama`
4. Ollama will start automatically as a background service

**Mac:**
1. Open the `.dmg` file
2. Drag Ollama to Applications folder
3. Open Applications â†’ Ollama
4. Ollama will start automatically

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### Step 3: Verify Installation

Open a **new terminal/command prompt** (anywhere, not in your project):

```bash
ollama --version
```

You should see version information.

### Step 4: Download Model (Still in Any Terminal)

```bash
ollama pull llama3.2:1b
```

**Model Location:**
- Windows: `C:\Users\YourName\.ollama\models\`
- Mac: `~/.ollama/models/`
- Linux: `~/.ollama/models/`

**You don't need to know this location** - Ollama manages it automatically!

### Step 5: Configure Your Project

**Go to your project folder:**
```
D:\desktop-top\Intent_search_Cine_Ai\
```

**Create `.env` file here** (in your project root):

**Windows PowerShell:**
```powershell
cd D:\desktop-top\Intent_search_Cine_Ai
echo "OLLAMA_URL=http://localhost:11434" > .env
echo "OLLAMA_MODEL=llama3.2:1b" >> .env
```

**Windows CMD:**
```cmd
cd D:\desktop-top\Intent_search_Cine_Ai
echo OLLAMA_URL=http://localhost:11434 > .env
echo OLLAMA_MODEL=llama3.2:1b >> .env
```

**Mac/Linux:**
```bash
cd /path/to/Intent_search_Cine_Ai
cat > .env << EOF
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:1b
EOF
```

---

## Directory Structure

### Your Project (Stays Where It Is)
```
D:\desktop-top\Intent_search_Cine_Ai\
â”œâ”€â”€ app.py
â”œâ”€â”€ rag_generator.py
â”œâ”€â”€ .env                    â† Create this here
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ vector_store.py
â”œâ”€â”€ rag_search.py
â”œâ”€â”€ semantic_search.py
â””â”€â”€ ... (all your project files)
```

### Ollama (System Installation - Separate)
```
C:\Users\YourName\AppData\Local\Programs\Ollama\    (Windows)
    â””â”€â”€ ollama.exe

C:\Users\YourName\.ollama\models\                    (Windows - Models)
    â””â”€â”€ llama3.2:1b\                                 (1.6 GB model)
```

**They are separate!** Your project connects to Ollama via `http://localhost:11434`

---

## How Connection Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your Project                       â”‚
â”‚  D:\desktop-top\Intent_search_Cine_Ai\ â”‚
â”‚                                     â”‚
â”‚  rag_generator.py                   â”‚
â”‚  â””â”€> Calls: http://localhost:11434  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTP Request
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama Service                     â”‚
â”‚  Running on: localhost:11434        â”‚
â”‚  Location: System-wide              â”‚
â”‚                                     â”‚
â”‚  â””â”€> Processes request              â”‚
â”‚  â””â”€> Uses model: llama3.2:1b        â”‚
â”‚  â””â”€> Returns response              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Important Points

### âœ… What You Need to Know

1. **Download Ollama** from https://ollama.ai/download
2. **Install it** (default location is fine)
3. **Download model** with `ollama pull llama3.2:1b`
4. **Create `.env` in your project** (not in Ollama folder)
5. **Start your project** - It will connect to Ollama automatically

### âŒ What You DON'T Need to Know

1. âŒ Exact Ollama installation path (not needed)
2. âŒ Model storage location (Ollama manages it)
3. âŒ How to configure Ollama (defaults work fine)
4. âŒ Moving files around (keep everything separate)

---

## Quick Setup Checklist

- [ ] Download Ollama from https://ollama.ai/download
- [ ] Install Ollama (use default location)
- [ ] Verify: `ollama --version` works
- [ ] Download model: `ollama pull llama3.2:1b`
- [ ] Verify: `ollama list` shows the model
- [ ] Go to your project: `D:\desktop-top\Intent_search_Cine_Ai\`
- [ ] Create `.env` file in project root
- [ ] Add: `OLLAMA_URL=http://localhost:11434`
- [ ] Add: `OLLAMA_MODEL=llama3.2:1b`
- [ ] Start your server: `uvicorn app:app --reload`
- [ ] Test RAG search in UI

---

## Troubleshooting

### "Ollama not found" after installation

**Windows:**
- Restart your terminal/command prompt
- Or add Ollama to PATH manually (usually not needed)

**Mac/Linux:**
- Restart terminal
- Or run: `source ~/.bashrc` (or `~/.zshrc`)

### "Cannot connect to Ollama"

**Check if Ollama is running:**
```bash
ollama list
```

**If not running:**
- Windows: Search "Ollama" in Start menu and open it
- Mac: Open Applications â†’ Ollama
- Linux: Run `ollama serve`

### Model location issues

**You don't need to know model location!** Ollama manages it automatically.

If you want to check:
```bash
ollama list  # Shows downloaded models
```

---

## Summary

1. **Download Ollama:** https://ollama.ai/download (anywhere is fine)
2. **Install Ollama:** Use default location (system-wide)
3. **Download Model:** `ollama pull llama3.2:1b` (any terminal)
4. **Configure Project:** Create `.env` in your project folder
5. **Connect:** Your project connects to Ollama via `localhost:11434`

**Key Point:** Ollama and your project are separate. They communicate via HTTP on `localhost:11434`. No need to put them in the same folder!

---

## Visual Guide

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Download Ollama                â”‚
â”‚  From: https://ollama.ai/download        â”‚
â”‚  Save to: Downloads folder (anywhere)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Install Ollama                 â”‚
â”‚  Location: System folder (automatic)     â”‚
â”‚  Windows: C:\Users\...\Programs\Ollama   â”‚
â”‚  Starts automatically as service         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Download Model                 â”‚
â”‚  Command: ollama pull llama3.2:1b       â”‚
â”‚  Location: ~/.ollama/models (automatic) â”‚
â”‚  Size: 1.6 GB                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Configure Your Project         â”‚
â”‚  Location: D:\desktop-top\Intent_search_Cine_Ai\ â”‚
â”‚  Create: .env file                      â”‚
â”‚  Content: OLLAMA_URL=http://localhost:11434 â”‚
â”‚           OLLAMA_MODEL=llama3.2:1b      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Start Your Project              â”‚
â”‚  Command: uvicorn app:app --reload      â”‚
â”‚  Your project connects to Ollama        â”‚
â”‚  via: http://localhost:11434            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Final Answer

**Where to download:** https://ollama.ai/download

**Where to install:** Default location (system folder) - you don't need to choose

**Where to configure:** Your project folder (`D:\desktop-top\Intent_search_Cine_Ai\`) - create `.env` here

**How they connect:** Via HTTP on `localhost:11434` - no need to be in same folder!

That's it! Simple and clean separation. ğŸš€

